\chapter{Avaliação}
\label{cap:evaluation}

Neste capítulo serão apresentadas as avaliações da solução proposta, metodologias utilizadas, conjunto de dados estudados, métricas e discussões sobre o significado dos resultado em relação aos objetivos inicialmente traçados. Espera-se que com o desenvolvimento de uma métrica de similaridade semântica, explorando as relações de recursos no DBPedia\footnote{http://wiki.dbpedia.org}, seja possível tirar vantagem para sugerir itens, invés da análise mais sintática do conteúdo utilizado em métodos como \ac{TFIDF}. Também é desejado verificar o impacto do uso da sinopse do filme, um dado não estruturado, invés de itens mais comuns como gênero, diretor, atores, com o objetivo de \enquote{fugir} das recomendações que prendam mais o usuário no mesmo tipo de filmes, mas ainda assim ser capaz de ser relevante aos seus interesses.

Inicialmente serão apresentados os dados utilizados e resultados iniciais do uso da métrica de similariade utilizada, analisando os efeitos desejados. Posteriormente o método de recomendação que utiliza a similaridade semântica apresentado na seção \ref{ssec:formula_rlws} será comparado com o método da similaridade do cosseno, utilizando-se métricas que serão definidas e apresentadas. O resultado esperado é de que utilizando um método que leve em consideração relações semânticas tenha melhores resultados daqueles que apenas possuem análises sintáticas. Por fim, serão abordadas discussões sobre resultados alcançados além de pontos de melhoria.

\section{Metodologia}
\label{sec:methodology}

Na avaliação de um sistema de recomendação é importante entender sua eficácia e seus algoritmos envolvidos, uma vez que uma análise incorreta pode levar subestimação ou superestimação da sua real precisão, como aponta \cite{Aggarwal2016:Evaluation}. Sendo assim, recomendadores podem ser avaliados tanto usando métodos denominados como \textit{online} ou \textit{offline}. Num sistema \textit{online} as opiniões e reações dos usuários são consideradas e medidas de acordo com as recomendações apresentadas, tendo a participação de fato, como algo crucial para a compreensão dos resultados. Contudo, como a avaliação desse método requer a participação do usuário, o que nem sempre é viável, e por isso também existe o método \textit{offline}, onde um conjunto de diferentes tipos de dados históricos dos usuários são utilizados \citep{Herlocker:1999}. Para os experimentos descritos nesta seção, serão utilizados em sua maioria um conjunto de dados offline retirados do projeto MovieLens\footnote{https://movielens.org}, conforme apresentado na seção \ref{sec:av_data_set}. Utilizando-se da mesma metodologia também serão avaliados resultados de usuários online, mas para um grupo bem pequeno, apenas para realizar uma comparação da diferença de resultados encontrados.

O objetivo dos experimentos que serão apresentados, é avaliar se a utilização da similaridade semântica junto ao método de recomendação proposto, é capaz de trazer resultados melhores nas métricas de avaliação em relação a similaridade do cosseno utilizando \ac{TFIDF}.  Os resultados tratam-se das análises das métricas extraídas das avaliações realizadas por usuários em relação as recomendações geradas por esses métodos.

Para realizar os testes entre os dois métodos de recomendação é necessário construir um perfil do usuário,
formado através dos filmes que avaliou. Cada usuário deve ter pelo menos 10 avaliações. Com intuito de captar os interesses do usuário, definiu-se que para montar o modelo do usuário com seus melhores termos, serão utilizados apenas os filmes com avaliação igual ou superior a 3,5, sendo 5 a avaliação máxima. No capítulo \ref{cap:proposal} foi definido que o perfil do usuário seria um conjunto $M_u$ dos melhores termos, possuindo um tamanho $z$ constante. Nos experimentos a seguir foi estipulado $z$ como 15 termos. A quantidade de termos definida possui um impacto grande na performance do sistema, uma vez que a complexidade do algoritmo da comparação entre termos é de $O(zm)$ (considerando $z$ como constante tem-se $O(m)$), sendo $m$ a quantidade de termos do filme. A Figura \ref{fig:tempo_x_termos} demonstra um gráfico da quantidade de termos em relação ao tempo de processamento, considerando que todos os dados estão no \textit{cache local}, conforme abordado no capítulo \ref{cap:proposal}, ou seja, o melhor caso.

\begin{figure}
	\centering
	\includegraphics[scale=0.85]{imagens/tempo_x_termos.jpg}
	\caption{Gráfico da relação gráfico da quantidade de termos em relação ao tempo de processamento. Execução numa máquina com processador \textit{i7 6700K}, 16GB RAM, \textit{Windows 10}.}
	\label{fig:tempo_x_termos}
\end{figure}

Definida a metodologia para a construção do perfil do usuário, na análise dos resultados serão realizados 3 experimentos para construir as recomendações. O total de recomendações possíveis trata-se de todos os outros filmes que o usuário não avaliou, o que torna extremamente trabalhosa a sua avaliação, portando indo contrário aos propósitos de um \ac{SR}, como filtrar e classificar resultados personalizados, poupando-o tempo na busca por informações. Sendo assim, apenas uma quantidade pequena de filmes serão recomendados, sendo um total de 20 recomendações com melhores scores, por experimento, uma vez que o importante é avaliar os bons primeiros resultados, ou aqueles exibidos primeiramente, pois conforme cada vez o usuário tem que continuar procurando por resultados, pior pode ser a percepção de relevância, conforme argumentado por \cite{Manning:2008}. 

Os três experimentos consistem nas seguintes abordagens:

\begin{enumerate}
	\item{\textbf{Experimento 1}: Serão construídas recomendações utilizando o método apresentado na proposta, utilizando a similaridade \ac{RLWS} com pesos 0,8 para links diretos e 0,2 para indiretos.}
	\item{\textbf{Experimento 2}: Semelhante ao experimento 1, mas com 0,2 para links diretos e 0,8 para indiretos.}
	\item{\textbf{Experimento 3}: Recomendações construídas utilizando o método da similaridade do cosseno.}
\end{enumerate}

O objetivo da variação dos pesos para a similaridade \ac{RLWS} é analisar o comportamento privilegiando o relacionamento direto ou indireto de recursos na DBPedia.
 
É importante ressaltar que para os testes \textit{online}, os usuários serão entrevistados, fornecendo um conjunto de 10 filmes de sua preferência, além de que não será detalhado qual método foi utilizado para as recomendações geradas por cada experimento. Para cada item recomendado o usuário terá de avaliar com uma nota de 0 a 5 estrelas, onde serão considerados os itens relevantes como aqueles que possuírem 3,5 estrelas ou mais. Já para os testes offline, como não há como saber a avaliação do usuário, será utilizada uma média das avaliações de outros usuários, onde também serão consideradas relevantes aquelas avaliações com média superior ou igual a 3,5 estrelas.

Para medir os resultados das recomendações dos experimentos serão utilizadas métricas como \textit{Precision} e \textit{Recall}, que dependem de um modelo de classificação binária \citep{Powers_2008}, sendo assim avaliações maiores ou iguais a 3,5 estrelas serão consideradas relevantes ou positivas, e inferiores como irrelevantes ou negativas. Na seção \ref{sec:av_metrics} serão melhor discutidas as métricas empregadas para avaliar os experimentos, assim como seus resultados.

Na seção seguinte será apresentado os dados trabalhados e gerados durante a execução dos experimentos.

\section{Conjunto de dados}
\label{sec:av_data_set}

Os dados usados durante os experimentos tratam-se de filmes , usuários com seus termos extraídos pelo processo do \ac{NLP}, além de avaliações. A Tabela \ref{tab:dataset} demonstra a quantidade de dados utilizados durante os testes. Note que os \enquote{URIs válidas} tratam-se termos extraídos das sinopses que possuem uma \ac{URI} associada a um recurso no DBPedia\footnote{http://wiki.dbpedia.org}. Os \enquote{usuários, teste offline} e \enquote{online} tratam-se da quantidade de usuários utilizados do total, em que foram geradas as recomendações dos experimentos.

\begin{table}[H]
\centering
\def\arraystretch{1.3}
\begin{tabular}{|l|l|}
	\hline
	\textbf{Dado}          & \textbf{Quantidades}   \\ \hline
	Filmes                 & 5.107                  \\ \hline
	Usuários               & 100.004                \\ \hline
	Usuários, teste offline & 30					\\ \hline
	Usuários, teste online & 4						\\ \hline
	Total de avaliações       & 11.997.970          \\ \hline
	Total URIs             & 22.977                 \\ \hline
	Total URIs válidas     & 18.629                 \\ \hline
	Total de comparações de URIs & 2.729.025        \\ \hline
\end{tabular}
\caption{Contagem dos dados utilizados durante os testes.}
\label{tab:dataset}
\end{table}

O total de URIs válidas é um ponto de contenção importante de ser analisado, uma vez que se o termo não se trata de um recurso na DBPedia, a comparação do \ac{RLWS} é descartada. Sendo assim, algo importante para a viabilidade da similaridade é de que a maioria dos termos extraídos das descrições dos filmes, tenha uma \ac{URI} associada ao recurso. A Tabela \ref{tab:lod_statistics} demonstra algumas estatísticas em relação aos termos extraídos dos filmes. Abaixo é descrito os conceitos construídos para a análise dos dados da tabela.

\begin{itemize}
	\item{\textbf{Cobertura DBPedia}: Trata-se do percentual dos termos encontrados que possuem uma \ac{URI} associada no DBPedia. É importante que este valor seja alto, pois caso não sejam encontrados os termos no serviço da web semântica a similaridade torna-se inválida.}
	
	\item{\textbf{Cobertura Links Diretos}: É o percentual de links que possuem pelo menos um relacionamento direto com outro termo dentro do conjunto de dados do experimento. Este valor revela o quão útil pode ser comparar dois termos quaisquer buscando por relacionamentos diretos no DBPedia, conforme abordado no capítulo \ref{cap:proposal}.}
	
	\item{\textbf{Cobertura Links Indiretos}: Similar à cobertura de links indiretos, mas agora sendo o percentual de links que possuem pelo menos um relacionamento indireto com outro termo.}
	
	\item{\textbf{Cobertura de Filmes com Links Diretos}: É o percentual de filmes contendo pelo menos um termo com ao menos um relacionamento direto a outro termo. Este valor tem o intuito de demonstrar a viabilidade da comparação utilizando a métrica \ac{RLWS} para comparar filmes utilizando-se os termos extraídos das suas descrições. A ideia é de que a maioria dos filmes possam utilizar a comparação de termos, tendo pelo menos um termo com relacionamento a outro encontrado.}
	
	\item{\textbf{Cobertura de Filmes com Links Indiretos}: O mesmo da cobertura de filmes com links diretos, mas agora sendo o percentual para os indiretos.}
\end{itemize}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Dado}                           & \textbf{Valor} \\ \hline
Cobertura DBPedia                       & 81,08\%        \\ \hline
Cobertura Links Diretos                 & 13,77\%        \\ \hline
Cobertura Links Indiretos               & 59,57\%        \\ \hline
Cobertura de Filmes com Links Diretos   & 99,96\%        \\ \hline
Cobertura de Filmes com Links Indiretos & 100,00\%       \\ \hline
\end{tabular}
\caption{Estatística da cobertura dos dados dos links de recursos na DBPedia}
\label{tab:lod_statistics}
\end{table}

É importante ressaltar que os dados das coberturas apenas consideram os \enquote{links válidos}, ou seja, aqueles que possuem uma URI associada no DBPedia. Nota-se também que os dados mencionados na tabela em relação a cobertura de links diretos e indiretos trata-se das comparações realizadas nos experimentos, sendo assim, não se completam, pois ainda existem links com relacionamentos que não foram comparados.

Algo relevante para destacar quanto à relação entre recursos é de que a quantidade de relacionamentos entre recursos cujo a quantidade links diretos é maior que 0, é de apenas 0,14\%, conforme constatado na Tabela  \ref{tab:lod_cache_stats} da quantização da relação entre recursos. Já era esperado que a maioria dos recursos não tivesse propriedades diretamente conectadas entre si, devido a variedade de comparações indiscriminada entre termos do usuário e termos dos filmes. Isso resulta numa tabela de dados altamente esparsa em relação ao cálculo a participação direta na equação \ac{RLWS}, o que por consequência leva a uma tabela de dados grande contendo diversas comparações zeradas. Quanto a proporção de relacionamentos indiretos maiores que 0 em relação ao total é de 8,81\%. Embora esses valores sejam baixos, o foco é de que para cada termo existam pelo menos um relacionamento a outro termo, seja direta ou indiretamente, e de que esse relacionamento esteja distribuído pela maioria dos filmes, algo que é evidenciado pelos dados da Tabela \ref{tab:lod_statistics}. Na tabela abaixo ainda são apresentados os de recursos relacionados que possuem a propriedade \textit{dbo:wikiPageRedirects}, ou seja, aqueles que o DBPedia resolve sua \ac{URI} como sendo a mesma, portanto para a similaridade considerados iguais.

\begin{table}[H]
\centering
\def\arraystretch{1.3}
\begin{tabular}{|l|l|}
\hline
\textbf{Dado}                                   & \textbf{Quantidades} \\ \hline
Relação entre recursos                          & 2.729.025            \\ \hline
Relação entre recursos, direto $> 0$ 			& 3.951                \\ \hline
Relação entre recursos, indireto $> 0$ 			& 240.629              \\ \hline
Relação entre recursos, redirecionados entre si & 77                   \\ \hline
Relação entre recursos, direto e indireto $= 0$ & 2.487.949            \\ \hline
\end{tabular}
\caption{Contagem da relação entre recursos utilizados durante os experimentos.}
\label{tab:lod_cache_stats}
\end{table}

\section{Métricas de avaliação}
\label{sec:av_metrics}

Existem diversas métricas que são usadas tanto em avaliações \textit{online} e \textit{offline}, mas as mais comuns são as de \textit{accuracy}, embora existam outras como \textit{user coverage}, \textit{novelty}, \textit{trust} \citep{Jannach:2010}. Para este trabalho foram utilizadas tanto avaliações \textit{online} e \textit{offline}, com métodos de precisão para avaliar as classificações da recomendações, como \textit{Precision} e \textit{Recall}. Abaixo serão abordados os conceitos das métricas de avaliação utilizadas nos experimentos.

\subsection{Precision}

Avaliando recomendações com métodos \textit{offline} apenas utilizando dados históricos da preferência do usuário, somente pode informar aqueles itens que foram de conhecimento do usuário, portando todos os outros itens terão de avaliados de outras formas, que não seja diretamente pela sua opinião, o que pode levar à falso positivos e/ou negativos. Por outro lado, avaliando com usuários reais, esses podem julgar todos os itens recomendados, podendo de fato definir se a predição foi correta ou não. Com a avaliação do usuário é possível construir uma tabela de classificação conforme a Figura \ref{fig:truth_table} \citep{Jannach:2010}, onde há cruzamento entre o que o recomendador apresentou e o que usuário avaliou. Se um item foi apresentado na recomendação e o usuário tenha gostado, avaliado como relevante, tem-se um caso de predição correta, ou \textit{true positive}. Outro resultado positivo, trata-se de quando o usuário não tenha gostado e o recomendador omitiu o resultado, ou seja uma omissão correta ou \textit{true negative}. Assim os resultados positivos estão na diagonal da esquerda para direita da tabela, e os resultados não desejados e negativos estão na outra diagonal. 

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{imagens/truth_table.jpg}
	\caption{Tabela de tipos de erros baseada na ilustração de \cite{Jannach:2010}.}
	\label{fig:truth_table}
\end{figure}

Considerando e classificando os resultados dessa forma binária, em positivos e negativos, defini-se \textit{precision}, precisão ou confiança, como sendo a fração de resultados previstos e avaliados pelo usuário como positivos, ou seja, os \textit{true positive}, em relação a quantidade de todos os itens recomendados \citep{Powers_2008}. A Equação \ref{eq:precision} demonstra o cálculo da precisão $P$, onde $tp$ trata-se da quantidade de itens \textit{true positive} e $fp$ como \textit{false positive}.

\begin{equation}
	P = \frac{tp}{tp + fp}
\label{eq:precision}
\end{equation}

Como a quantidade de resultados pode ser muito grande para calcular a precisão, e até para que o próprio usuário o faça, por extensão também defini-se como $P@k$, como sendo a precisão até $k$ primeiros resultados retornados pelo recomendador \citep{Aggarwal2016:Evaluation}. A Equação \ref{eq:precision_at_k} demonstra a variação do cálculo de $P$ onde $r$ trata-se da quantidade de itens relevantes até o \textit{rank} $k$. \ref{eq:precision_at_k}.

\begin{equation}
	p@k = \frac{r}{k}
\label{eq:precision_at_k}
\end{equation}

\subsection{Recall}

Outra métrica de precisão utilizando classificações binárias é o \textit{recall}, que trata-se da proporção de resultados \textit{true positive} em relação ao total possível de positivos reais avaliados pelo usuário \citep{Powers_2008}. Os valores irão progredir de 0 a 1 sempre, sendo o valor 1 atribuído para todos os elementos a partir (inclusive) do último item previsto como \textit{true positive}. O objetivo além de verificar quais os itens mais relevantes, é constatar o quão melhor eles se posicionam na ordem dos retornados, sendo o ideal é de que mais itens como \textit{true positive} estejam nos primeiros resultados \citep{Jannach:2010}. A Equação \ref{eq:recall} demonstra o cálculo do \textit{recall} $R$, onde $tp$ trata-se da quantidade de itens \textit{true positive} e $fn$ como \textit{false negative}.

\begin{equation}
	R = \frac{tp}{tp + fn}
\label{eq:recall}
\end{equation}

Também por extensão podemos calcular $R@k$ como sendo o \textit{recall} até os $k$ primeiros resultados. Assim, primeiro obtém-se o total de itens relevantes até $k$ para ser utilizado como denominador da equação do original do \textit{recall}.

\subsection{Mean Average Precision (MAP)}

Outra métrica de precisão trata-se da \ac{MAP}, que busca estipular um único valor de precisão em relação ao conjunto de avaliações de múltiplos usuários \citep{Manning:2008}. A equação \ref{eq:map} demonstra o cálculo, onde $AveP(u)$ trata-se da média das precisões $p@k$ do usuário $u \in U$, onde $|R|$ é quantidade de itens relevantes até $k$-ésimo \textit{rank}. Posteriormente obtém-se o $MAP$ como sendo a média $AveP$ para todos os usuários avaliados. A Figura \ref{fig:map_ex} exemplifica o cálculo da métrica, onde os quadrados \enquote{verdes} representam os itens \textit{true positive} e os itens vermelhos os \textit{false positive}.

\begin{equation}
	AveP(u) = \frac{\sum_{k=1}^{n} p@k}{|R|}, \; u \in U
\label{eq:avep}
\end{equation}

\begin{equation}
	MAP = \frac{\sum_{u=1}^{|U|}AveP(u)}{|U|}
\label{eq:map}
\end{equation}

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{imagens/map_ex.jpg}
	\caption{Exemplo do cálculo do MAP.}
	\label{fig:map_ex}
\end{figure}

\subsection{Mean Reciprocal Rank (MRR)}

A métrica \ac{MRR} trata-se da média da classificação recíproca (\textit{reciprocal rank}) de cada usuário \citep{Burges:2006}, sendo esta o multiplicativo inverso a posição do primeiro item correto no \textit{rank} de recomendações, ou top-N itens. O objetivo é obter um valor geral que informe o quão longe o primeiro resultado positivo está do primeiro item. A Equação \ref{eq:mrr} demonstra o cálculo onde $\frac{1}{k}$ trata-se do \textit{reciprocal rank} até o $k$-ésimo item. A Figura \ref{fig:mrr_ex} exemplifica o cálculo da métrica, onde os quadrados \enquote{verdes} representam os itens \textit{true positive} e os itens vermelhos os \textit{false positive}.

\begin{equation}
	MRR = \frac{1}{|U|} \sum_{u=1}^{|U|} \frac{1}{k}, \; u \in U
\label{eq:mrr}
\end{equation}

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{imagens/mrr_ex.jpg}
	\caption{Exemplo do cálculo do MRR.}
	\label{fig:mrr_ex}
\end{figure}

\section{Resultados}
\label{sec:results}

Antes de apresentar os resultados das recomendações com as avaliações dos usuários, é importante avaliar algumas premissas e comportamentos da própria equação de similaridade, a \ac{RLWS}. Inicialmente o esperado é de que recursos que sejam intuitivamente próximos, ou provavelmente tenham diversas relações entre si,  como \textit{Earth} e \textit{Moon}, possuam maior similaridade do que \textit{Earth} e \textit{Table}. E de fato, mesmo nos dois extremos de pesos, seja priorizando as links diretos ou indiretos, existe uma diferença considerável quando termos estão intuitivamente mais próximos do que aqueles que provavelmente não terão relacionamentos em comum, conforme mostra Figura \ref{tab:rlws_results}. Note que \textit{RLWS(0,8/0,2)} refere-se ao uso dos pesos como sendo o primeiro 0,8 para links diretos e o segundo 0,2 para indiretos.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Termo 1} & \textbf{Termo 2}    & \textbf{RLWS (0,8/0,2)} & \textbf{RLWS (0,2/0,8)} \\ \hline
France           & Paris               & 0,434                   & 0,858                   \\ \hline
France           & Juice               & 0,111                   & 0,443                   \\ \hline
France           & Art                 & 0,190                   & 0,760                   \\ \hline
Brazil           & Brasilia            & 0,193                   & 0,770                   \\ \hline
Brazil           & Box                 & 0,050                   & 0,200                   \\ \hline
Brazil           & Paper               & 0,163                   & 0,652                   \\ \hline
Brazil           & Beach               & 0,282                   & 0,726                   \\ \hline
Car              & Automobile          & 1,0                     & 1,0                     \\ \hline
United\_States   & Washington,\_D,C,   & 0,372                   & 0,842                   \\ \hline
China            & Hong\_Kong          & 0,377                   & 0,842                   \\ \hline
Ariana\_Grande   & Selena\_Gomez       & 0,320                   & 0,800                   \\ \hline
Selena\_Gomez    & Elon\_Musk          & 0,022                   & 0,087                   \\ \hline
Coconut          & Plant               & 0,393                   & 0,683                   \\ \hline
Tom\_Cruise      & Lady\_Gaga          & 0,162                   & 0,646                   \\ \hline
Star             & Galaxy              & 0,339                   & 0,809                   \\ \hline
Earth            & Moon                & 0,485                   & 0,866                   \\ \hline
Earth            & Table               & 0,033                   & 0,132                   \\ \hline
Book             & Movie               & 0,125                   & 0,500                   \\ \hline
Book             & Metal               & 0,096                   & 0,386                   \\ \hline
Johnny\_Cash     & June\_Carter\_Cash  & 0,579                   & 0,868                   \\ \hline
Johnny\_Cash     & Al\_Green           & 0,176                   & 0,705                   \\ \hline
Johnny\_Cash     & Elvis\_Presley      & 0,316                   & 0,816                   \\ \hline
Johnny\_Cash     & Kris\_Kristofferson & 0,317                   & 0,804                   \\ \hline
Johnny\_Cash     & Carlene\_Carter     & 0,457                   & 0,743                   \\ \hline
\end{tabular}
\caption{Tabela de amostra de comparações entre termos usando \ac{RLWS}.}
\label{tab:rlws_results}
\end{table}

É importante ressaltar que mesmo para termos que estejam aparentemente mais distantes, como \textit{Selena\_Gomez} e \textit{Ariana\_Grande}, por se tratarem de "coisas" que não são imediatamente próximas, ainda possuem uma alta similaridade, devido as conexões que ambas as pessoas possuem quanto ao domínio da música e aparições em temas de filmes, programas etc. Já quando compara-se \textit{Selena\_Gomez} com \textit{Elon\_Musk}, mesmo sendo pessoas já possuem uma similaridade bem menor, o que também é intuitivamente esperado. Também se observa na comparação \textit{Johnny\_Cash} e \textit{June\_Carter\_Cash}, uma alta similaridade, pois os dois foram casados e cantores. Outra observação importante é quanto aos termos \textit{Car} e \textit{Automobile} que possuem similaridade 1. Isso é devido que os dois possuem a propriedade \textit{dbo:wikiPageRedirect} conectado seus recursos, o que por regra entra na clausula da equação tendo valor 1. Esses redirecionamentos também ocorrem nos  termos \textit{Future} e \textit{Futuristic}, \textit{Power} e \textit{Powerful} entre outros. Para uma série de termos esses redirecionamentos contribuem bastante para o desempenho da equação, devido a sua real proximidade, apesar de serem termos diferentes.

Nota-se que intuitivamente os resultados das comparações fazem sentido tanto usando pesos que privilegiam links diretos ou indiretos, o que é vital para coerência no momento da comparação termo a termo. Outro fato importante é a consideração de itens que possuem redirecionamentos, ainda que sejam raros, mas para palavras como "Carro" e "Automóvel" é sensato dizer que são similares.

Avaliadas as premissas e os resultados para a similaridade semântica proposta, em sequência serão calculados as métricas mencionadas na seção \ref{sec:av_metrics}, medindo assim o impacto do modelo de recomendação assim como a própria similaridade \ac{RLWS}. Os testes dos experimentos mencionados na seção \ref{sec:methodology} serão divididos em dois resultados, os testes com dados de \enquote{usuários offline} e \enquote{usuários offline}.

\subsection{MAP}

Para medir a assertividade do \ac{SR} foi utilizada a métrica \ac{MAP} perante dois conjuntos de usuários, os dos testes \textit{offline} (sendo no total de 30) e os \textit{online} (com total de 4). 

\section{Discussão dos resultados}
