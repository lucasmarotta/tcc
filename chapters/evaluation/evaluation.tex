\chapter{Avaliação}
\label{cap:evaluation}

Neste capítulo serão apresentadas as avaliações da solução proposta, metodologias utilizadas, conjunto de dados estudados, métricas e discussões sobre o significado dos resultado em relação aos objetivos inicialmente traçados. Espera-se que com o desenvolvimento de uma métrica de similaridade semântica, explorando as relações de recursos no DBPedia\footnote{http://wiki.dbpedia.org}, seja possível tirar vantagem para sugerir itens, invés da análise mais sintática do conteúdo utilizado em métodos como \ac{TFIDF}. Também é desejado verificar o impacto do uso da sinopse do filme, um dado não estruturado, invés de itens mais comuns como gênero, diretor, atores, com o objetivo de "fugir" das recomendações que prendam mais o usuário no mesmo tipo de filmes, mas ainda assim ser capaz de ser relevante aos seus interesses.

Inicialmente serão apresentados os dados utilizados e resultados iniciais do uso da métrica de similariade utilizada, analisando os efeitos desejados. Posteriormente o método de recomendação que utiliza a similaridade semântica apresentado na seção \ref{ssec:formula_rlws} será comparado com o método da similaridade do cosseno, utilizando-se métricas que serão definidas e apresentadas. O resultado esperado é de que utilizando um método que leve em consideração relações semânticas tenha melhores resultados daqueles que apenas possuem análises sintáticas. Por fim, serão abordadas discussões sobre resultados alcançados.

\section{Metodologia}

O objetivo dos testes que serão apresentados, é avaliar se a utilização da similaridade semântica junto ao método de recomendação proposto, é capaz de trazer resultados melhores nas métricas de avaliação em relação a similaridade do cosseno utilizando \ac{TFIDF}.  Os resultados tratam-se das análises das métricas extraídas das avaliações realizadas por usuários em relação as recomendações geradas por esses métodos.

Para realizar os testes entre os dois métodos de recomendação o usuário deverá construir um perfil, contendo 10 filmes de preferência e em seguida o sistema gerará 3 listas de filmes recomendados. O total de recomendações possíveis trata-se de todos os outros filmes que o usuário não escolheu, o que torna extremamente trabalhosa a sua avaliação, portando indo contrário aos propósitos de um \ac{SR}, como filtrar e classificar resultados personalizados, poupando-o tempo na busca por informações. Sendo assim, apenas uma quantidade pequena de filmes serão avaliados, sendo um total de 20 recomendações por lista, uma vez que o importante é avaliar os bons primeiros resultados, ou aqueles exibidos numa primeira página, pois conforme cada vez o usuário tem que continuar procurando por resultados, pior pode ser a percepção de relevância, conforme argumentado por \cite{Manning:2008}. 

As três primeiras listas tratam-se de variantes da recomendação utilizando \ac{RLWS}, aplicando-se pesos diferentes na equação, conforme a seguir: 

\begin{itemize}
	\item{A primeira com 0,8 para links diretos e 0,2 para indiretos.}
	\item{A segunda com 0,2 para links diretos e 0,8 para indiretos.}
\end{itemize}

O objetivo da variação dos pesos é analisar o comportamento privilegiando o relacionamento direto ou indireto. Por último tem a terceira lista de recomendações que é obtida pela similaridade do cosseno. É importante ressaltar que o usuário não terá conhecimento da diferença dos métodos utilizados em cada lista. No conjunto de filmes recomendados o usuário deverá avaliar a recomendação com uma nota entre 0 a 5 estrelas, sendo 0 totalmente irrelevante e 5 totalmente relevante. Para as avaliações dos usuários, serão utilizadas métricas como \textit{Precision} e \textit{Recall}, que dependem de um modelo de classificação binária \citep{Powers_2008}, sendo assim avaliações maiores ou iguais a 3 estrelas serão consideradas relevantes ou positivas, e inferiores como irrelevantes ou negativas. As avaliações dos usuários serão realizadas de forma manual por cada convidado a utilizar o sistema e participar do experimento de recomendação.

\section{Conjunto de dados}

Os dados tratados durante os testes do \ac{SR} tratam-se de filmes e usuários com seus termos extraídos pelo processo do \ac{NLP}, além do caso do usuário a utilização do método da escolha de melhores termos como visto na seção \ref{ssec:user_model}. A Tabela \ref{tab:dataset} demonstra a quantidade de dados utilizados durante os testes. Note que os "recursos válidos" tratam-se termos extraídos das sinopses que possuem recurso associado no DBPedia\footnote{http://wiki.dbpedia.org}. Este é um ponto de contenção importante de ser analisado, uma vez que se o termo não se trata de um recurso no DBPedia, a comparação do \ac{RLWS} torna-se inútil. Sendo assim, algo importante para a viabilidade da similaridade é de que a maioria dos termos extraídos das descrições dos filmes, sejam recursos, o que neste caso notamos de mais de 80\% de fato são válidos.

\begin{table}[H]
\centering
\def\arraystretch{1.3}
\begin{tabular}{|l|l|}
	\hline
	\textbf{Dado}          & \textbf{Quantidades}   \\ \hline
	Filmes                 & 5.107                  \\ \hline
	Usuários               & - 						\\ \hline
	Recursos               & 22.959                 \\ \hline
	Recursos válidos       & 18.611                 \\ \hline
	Relação entre recursos & 780.192                \\ \hline
\end{tabular}
\caption{Contagem dos dados utilizados durante os testes.}
\label{tab:dataset}
\end{table}

Os dados dos filmes foram extraídos do projeto MovieLens\footnote{https://movielens.org}, que possibilita a integração com a plataforma do IMDB\footnote{https://imdb.com}, uma vez que o mesmo também provê identificadores para esse serviço de banco de dados de filmes. Quanto aos dados do usuário, são em sua maioria gerados pela interação com sistema, partindo do seu cadastro que pode ser realizado pela própria plataforma, ou através do login pelo Facebook\footnote{https://facebook.com}. Essa opção de login facilita a coleta de dados como email, nome e até preferência de filmes, desde que estejam cadastrados no sistema. Após o login o sistema coletará dados da preferência do usuário seja parcialmente vindos pelo Facebook ou através da seleção pelo próprio sistema. Por fim o sistema gera recomendações para o usuário que são persistidas para posterior coleta das suas avaliações.

Para a montagem do perfil do usuário definiu-se que o conjunto de termos do usuário $M_u$ terá um tamanho $z$ constante. Nos testes foi estipulado $z$ como 15 termos escolhidos para a montagem do perfil. A quantidade de termos definida possui um impacto grande na performance do sistema, uma vez que a complexidade do algoritmo da comparação entre termos é de $O(zm)$ (considerando $z$ como constante tem-se $O(m)$), sendo $m$ a quantidade de termos do filme. A Figura \ref{fig:tempo_x_termos} demonstra um gráfico da quantidade de termos em relação ao tempo de processamento, considerando que todos os dados estão no \textit{cache} local, conforme abordado em \ref{cap:proposal}, ou seja, o melhor caso.

\begin{figure}
	\centering
	\includegraphics[scale=0.85]{imagens/tempo_x_termos.jpg}
	\caption{Gráfico da relação gráfico da quantidade de termos em relação ao tempo de processamento. Execução numa máquina com processador \textit{i7 6700K}, 16GB RAM, \textit{Windows 10}.}
	\label{fig:tempo_x_termos}
\end{figure}

Algo relevante para destacar quanto à relação entre recursos é de que a quantidade links diretos entre dois recursos maior que 0 é altamente rara, sendo apenas de 0,15\%. Já era esperado que a maioria dos recursos não tivesse propriedades diretamente conectadas entre si, devido a variedade de comparações indiscriminada entre termos do usuário e termos dos filmes. Isso resulta numa tabela de dados altamente esparsa em relação ao cálculo a participação direta na equação \ac{RLWS}. A proporção de relacionamentos indiretos maiores que 0 em relação ao total é de 10,6\%. O quadro da Tabela \ref{tab:lod_cache_stats} apresenta outros resultados sobre os recursos, note que os redirecionados tratam-se de recursos relacionados que possuem a propriedade \textit{dbo:wikiPageRedirects}, ligando um ao outro.

\begin{table}[H]
\centering
\def\arraystretch{1.3}
\begin{tabular}{|l|l|}
\hline
\textbf{Dado}                                   & \textbf{Quantidades} \\ \hline
Relação entre recursos                          & 780.192              \\ \hline
Relação entre recursos, direto $> 0$ 			& 1.077                \\ \hline
Relação entre recursos, indireto $> 0$ 			& 74.928               \\ \hline
Relação entre recursos, redirecionados entre si & 21                   \\ \hline
Relação entre recursos, direto e indireto $= 0$ & 705.152              \\ \hline
\end{tabular}
\caption{Contagem dos dados utilizados durante os testes.}
\label{tab:lod_cache_stats}
\end{table}

\section{Métricas de avaliação}

O estudo da avaliação de \ac{SR} é importante para entender sua eficácia e seus algoritmos envolvidos, uma vez que uma análise incorreta pode levar subestimação ou superestimação da sua real precisão, como aponta \cite{Aggarwal2016:Evaluation}. Sendo assim, recomendadores podem ser avaliados tanto usando métodos denominados como \textit{online} ou \textit{offline}. Num sistema \textit{online} as opiniões e reações dos usuários são consideradas e medidas de acordo com as recomendações apresentadas, tendo a participação de fato, como algo crucial para a compreensão dos resultados. Contudo, como a avaliação desse método requer a participação do usuário, o que nem sempre é viável, também existe o método \textit{offline}, onde um conjunto de diferentes tipos de dados históricos dos usuários são utilizados \citep{Herlocker:1999}.

Existem diversas métricas que são usadas tanto em avaliações \textit{online} e \textit{offline}, mas as mais comuns são as de \textit{accuracy}, embora existam outras como \textit{user coverage}, \textit{novelty}, \textit{trust} \citep{Jannach:2010}. Para este trabalho foi utilizada uma avaliação \textit{online} utilizando métodos de precisão para avaliar as classificações da recomendações, como \textit{Precision} e \textit{Recall}.

\subsection{Precision}

Avaliando recomendações com métodos \textit{offline} apenas utilizando dados históricos da preferência do usuário, somente pode informar aqueles itens que foram de conhecimento do usuário, portando todos os outros itens serão considerados como avaliações negativas que o usuário não tem interesse, podendo levar à falso positivos. Por outro lado, avaliando com usuários reais, esses podem julgar todos os itens recomendados, podendo de fato definir se a predição foi correta ou não. Com a avaliação do usuário é possível construir uma tabela de classificação conforme a Figura \ref{fig:truth_table} \citep{Jannach:2010}, onde há cruzamento entre o que o recomendador apresentou e o que usuário avaliou. Se um item foi apresentado na recomendação e o usuário tenha gostado, avaliado como relevante, tem-se um caso de predição correta, ou \textit{true positive}. Outro resultado positivo, trata-se de quando o usuário não tenha gostado e o recomendador omitiu o resultado, ou seja uma omissão correta ou \textit{true negative}. Assim os resultados positivos estão na diagonal da esquerda para direita da tabela, e os resultados não desejados e negativos estão na outra diagonal. 

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{imagens/truth_table.jpg}
	\caption{Tabela de tipos de erros retirada de \cite{Jannach:2010}.}
	\label{fig:truth_table}
\end{figure}

Considerando e classificando os resultados dessa forma binária, em positivos e negativos, defini-se \textit{precision}, precisão ou confiança, como sendo a fração de resultados previstos e avaliados pelo usuário como positivos, ou seja, os \textit{true positive}, em relação a quantidade de todos os itens recomendados \citep{Powers_2008}. A Equação \ref{eq:precision} demonstra o cálculo da precisão $P$, onde $tp$ trata-se da quantidade de itens \textit{true positive} e $fp$ como \textit{false positive}.

\begin{equation}
	P = \frac{tp}{tp + fp}
\label{eq:precision}
\end{equation}

Como a quantidade de resultados pode ser muito grande para calcular a precisão, e até para que o próprio usuário o faça, por extensão também defini-se como $P@k$, como sendo a precisão até $k$ primeiros resultados retornados pelo recomendador \citep{Aggarwal2016:Evaluation}.

\subsection{Recall}

Outra métrica de precisão utilizando classificações binárias é o \textit{recall}, que trata-se da proporção de resultados \textit{true positive} em relação ao total possível de positivos reais avaliados pelo usuário \citep{Powers_2008}. Os valores irão progredir de 0 a 1 sempre, sendo o valor 1 atribuído para todos os elementos a partir (inclusive) do último item previsto como \textit{true positive}. O objetivo além de verificar quais os itens mais relevantes, é constatar o quão melhor eles se posicionam na ordem dos retornados, sendo o ideal é de que mais itens como \textit{true positive} estejam nos primeiros resultados \citep{Jannach:2010}. A Equação \ref{eq:recall} demonstra o cálculo do \textit{recall} $R$, onde $tp$ trata-se da quantidade de itens \textit{true positive} e $fn$ como \textit{false negative}.

\begin{equation}
	R = \frac{tp}{tp + fn}
\label{eq:recall}
\end{equation}

Também por extensão podemos calcular $R@k$ como sendo o \textit{recall} até os $k$ primeiros resultados. Assim, primeiro obtém-se o total de itens relevantes até $k$ para ser utilizado como denominador da equação do original do \textit{recall}.

\subsection{Mean Average Precision (MAP)}

Outra métrica de precisão trata-se da \ac{MAP}, que busca estipular um único valor de precisão em relação ao conjunto de avaliações de múltiplos usuários \citep{Manning:2008}. A equação \ref{eq:map} demonstra o cálculo, onde $AveP(n)$ trata-se da média das precisões do $n$ ésimo usuário, e $N$ a quantidade de usuários que realizaram a avaliação.

\begin{equation}
	MAP = \frac{\sum_{n=1}^{N}AveP(n)}{N}
\label{eq:map}
\end{equation}

\section{Resultados}

Antes de apresentar os resultados das recomendações com as avaliações dos usuários, é importante verificar algumas premissas e comportamentos da própria equação de similaridade, a \ac{RLWS}. Inicialmente o esperado que recursos que sejam intuitivamente próximos, ou provavelmente tenham diversas relações entre si,  como \textit{Earth} e \textit{Moon}, possum maior similaridade do que \textit{Earth} e \textit{Table}. E de fato, mesmo nos dois extremos de pesos, seja priorizando as ligações diretas ou indiretas, existe uma diferença considerável quando termos estão intuitivamente mais próximos do que aqueles que provavelmente não terão relacionamentos em comum, conforme mostra Figura \ref{tab:rlws_results}. Note que \textit{RLWS(0,8/0,2)} refere-se ao uso dos pesos como sendo o primeiro 0,8 para links diretos e o segundo 0,2 para indiretos.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Termo 1} & \textbf{Termo 2}    & \textbf{RLWS (0,8/0,2)} & \textbf{RLWS (0,2/0,8)} \\ \hline
France           & Paris               & 0,434                   & 0,858                   \\ \hline
France           & Juice               & 0,111                   & 0,443                   \\ \hline
France           & Art                 & 0,190                   & 0,760                   \\ \hline
Brazil           & Brasilia            & 0,193                   & 0,770                   \\ \hline
Brazil           & Box                 & 0,050                   & 0,200                   \\ \hline
Brazil           & Paper               & 0,163                   & 0,652                   \\ \hline
Brazil           & Beach               & 0,282                   & 0,726                   \\ \hline
Car              & Automobile          & 1,0                     & 1,0                     \\ \hline
United\_States   & Washington,\_D,C,   & 0,372                   & 0,842                   \\ \hline
China            & Hong\_Kong          & 0,377                   & 0,842                   \\ \hline
Ariana\_Grande   & Selena\_Gomez       & 0,320                   & 0,800                   \\ \hline
Selena\_Gomez    & Elon\_Musk          & 0,022                   & 0,087                   \\ \hline
Coconut          & Plant               & 0,393                   & 0,683                   \\ \hline
Tom\_Cruise      & Lady\_Gaga          & 0,162                   & 0,646                   \\ \hline
Star             & Galaxy              & 0,339                   & 0,809                   \\ \hline
Earth            & Moon                & 0,485                   & 0,866                   \\ \hline
Earth            & Table               & 0,033                   & 0,132                   \\ \hline
Book             & Movie               & 0,125                   & 0,500                   \\ \hline
Book             & Metal               & 0,096                   & 0,386                   \\ \hline
Johnny\_Cash     & June\_Carter\_Cash  & 0,579                   & 0,868                   \\ \hline
Johnny\_Cash     & Al\_Green           & 0,176                   & 0,705                   \\ \hline
Johnny\_Cash     & Elvis\_Presley      & 0,316                   & 0,816                   \\ \hline
Johnny\_Cash     & Kris\_Kristofferson & 0,317                   & 0,804                   \\ \hline
Johnny\_Cash     & Carlene\_Carter     & 0,457                   & 0,743                   \\ \hline
\end{tabular}
\caption{Tabela de amostra de comparações entre termos usando \ac{RLWS}.}
\label{tab:rlws_results}
\end{table}

É importante ressaltar que mesmo para termos que estejam aparentemente mais distantes, como \textit{Selena\_Gomez} e \textit{Ariana\_Grande}, por se tratarem de "coisas" que não são imediatamente próximas, ainda possuem uma alta similaridade, devido as conexões que ambas as pessoas possuem quanto ao domínio da música e aparições em temas de filmes, programas etc. Já quando compara-se \textit{Selena\_Gomez} com \textit{Elon\_Musk}, mesmo sendo pessoas já possuem uma similaridade bem menor, o que também é intuitivamente esperado. Também se observa na comparação \textit{Johnny\_Cash} e \textit{June\_Carter\_Cash}, uma alta similaridade, pois os dois foram casados e cantores. Outra observação importante é quanto aos termos \textit{Car} e \textit{Automobile} que possuem similaridade 1. Isso é devido que os dois possuem a propriedade \textit{dbo:wikiPageRedirect} conectado seus recursos, o que por regra entra na clausula da equação tendo valor 1. Esses redirecionamentos também ocorrem nos  termos \textit{Future} e \textit{Futuristic}, \textit{Power} e \textit{Powerful} entre outros. Para uma série de termos esses redirecionamentos contribuem bastante para o desempenho da equação, devido a sua real proximidade, apesar de serem termos diferentes.

Nota-se que intuitivamente os resultados das comparações fazem sentido tanto usando pesos que privilegiam links diretos ou indiretos, o que é vital para coerência no momento da comparação termo a termo. Outro fato importante é a consideração de itens que possuem redirecionamentos, ainda que sejam raros, mas para palavras como "Carro" e "Automóvel" é sensato dizer que são similares.

\subsection{Resultados das recomendações}

\section{Discussão dos resultados}
